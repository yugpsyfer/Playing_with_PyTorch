{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px3CHzl0guxv",
        "outputId": "7788f9d4-ba00-456d-fba5-efc9140f1ea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torchmetrics import Accuracy, BLEUScore\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C0ct3UCk7HNM"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fKGMuywajZ0V"
      },
      "outputs": [],
      "source": [
        "class CharacterTokenizer:\n",
        "  def __init__(self, path):    \n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        self.text = f.read()\n",
        "\n",
        "    self.chars = sorted(list(set(self.text)))\n",
        "    self.vocab_size = len(self.chars)\n",
        "\n",
        "    self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
        "    self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
        "    \n",
        "  def encode(self):\n",
        "    raw_data = []\n",
        "    \n",
        "    for char in self.text:\n",
        "      raw_data.append(self.stoi[char])\n",
        "    \n",
        "    return raw_data\n",
        "\n",
        "  def decode(self, encoded_data):\n",
        "    raw_data = []\n",
        "  \n",
        "    for char_enc in encoded_data:\n",
        "      raw_data.append(self.itos[char_enc])\n",
        "    \n",
        "    return raw_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TPpjX1fxkTHz"
      },
      "outputs": [],
      "source": [
        "tokenizer = CharacterTokenizer('./input.txt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4QrdIBd47T5c"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SJnl1ISPhKcT"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(tokenizer.encode(), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V2XzJUglP5a"
      },
      "source": [
        "### Batch maker (Basically an adhoc DataLoader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WB5usCxAlPf5"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD_sJV316mjw"
      },
      "source": [
        "### Feed Forward Network which will come after MultiHead SA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3oXbUGzRllKb"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "  def __init__(self, n_embedding, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fn = nn.Sequential(\n",
        "        nn.Linear(n_embedding, 4 * n_embedding),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embedding, n_embedding),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fn(x)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5wGOnke66ZLu"
      },
      "source": [
        "### PyTorch inbuilt MultiHeadAttention and its TransformerBlock class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQKPGigjn6dI"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, n_embedding, n_heads, dropout, block_size=None) -> None:\n",
        "    super().__init__()\n",
        "    \n",
        "    self.self_attention = nn.MultiheadAttention(embed_dim=n_embedding,\n",
        "                                                 num_heads=n_heads,\n",
        "                                                 device=device,\n",
        "                                                 dropout=dropout)  # simple multihead attention from pytorch\n",
        "    \n",
        "    self.feed_forward = FeedForwardNetwork(n_embedding, dropout)\n",
        "    self.layer_norm_1 = nn.LayerNorm(n_embedding)\n",
        "    self.layer_norm_2 = nn.LayerNorm(n_embedding)\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    o = self.layer_norm_1(x)\n",
        "    o, _ = self.self_attention(o,o,o)\n",
        "    x = x + o\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1qlYg0nT03H"
      },
      "source": [
        "### Self attention built from scratch and its own TransformerBlock class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tTy37FaGH5KX"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "  def __init__(self, n_embedding, head_size, block_size, dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.Query = nn.Linear(in_features=n_embedding, out_features=head_size, bias=False)\n",
        "    self.Key = nn.Linear(in_features=n_embedding, out_features=head_size, bias=False)\n",
        "    self.Value = nn.Linear(in_features=n_embedding, out_features=head_size, bias=False)\n",
        "\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))   # I thought of using a no gradient tensor but this is better\n",
        "\n",
        "  def forward(self, x):\n",
        "    Batch, Time, Channel = x.shape\n",
        "\n",
        "    q = self.Query(x)\n",
        "    k = self.Key(x)\n",
        "    v = self.Value(x)\n",
        "\n",
        "    dot_prod = q @ k.transpose(-2, -1) * k.shape[-1]**0.5 # Stability - basically 1 variance (Evenly distributed)\n",
        "    dot_prod = dot_prod.masked_fill(self.tril[:Time, :Time]==0, float('-inf'))  # Bars the attention from looking ahead\n",
        "    dot_prod = F.softmax(dot_prod, dim=-1)\n",
        "    dot_prod = self.dropout(dot_prod)\n",
        "\n",
        "    out = dot_prod @ v\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Pb7pzjGqRWKk"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, n_heads, n_embedding, head_size, block_size, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention_blocks = nn.ModuleList([SelfAttentionHead(n_embedding, head_size, block_size, dropout) for _ in range(n_heads)])\n",
        "    self.projection_layer = nn.Linear(in_features=n_heads*head_size, out_features=n_embedding)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    out = torch.cat([h(x) for h in self.attention_blocks], dim=-1)  # concatenated vectors from each attention head\n",
        "    out = self.dropout(self.projection_layer(out)) # Finally getting a vector of same size as n_embedding\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XQAO7d-STB05"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, n_embedding, n_heads, dropout, block_size) -> None:\n",
        "    super().__init__()\n",
        "    \n",
        "    head_size = n_embedding//n_heads\n",
        "    \n",
        "    self.self_attention = MultiHeadSelfAttention(n_heads, n_embedding, head_size, block_size, dropout)\n",
        "\n",
        "    self.feed_forward = FeedForwardNetwork(n_embedding, dropout)\n",
        "    self.layer_norm_1 = nn.LayerNorm(n_embedding)\n",
        "    self.layer_norm_2 = nn.LayerNorm(n_embedding)\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    o = self.layer_norm_1(x)\n",
        "    o = self.self_attention(o)\n",
        "    x = x + o  # Residual connection\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x)) # Residual connection\n",
        "\n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jLm4nRYQ6hmY"
      },
      "source": [
        "### Nano GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3hQI7ZgDqVsO"
      },
      "outputs": [],
      "source": [
        "class NanoGPT(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embedding, block_size, n_heads, n_layers, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embeddding_lookup = nn.Embedding(vocab_size, n_embedding)\n",
        "    self.block_embedding_lookup = nn.Embedding(block_size, n_embedding)  #basically postional information\n",
        "\n",
        "    self.transformer_blocks = nn.Sequential(*[TransformerBlock(n_embedding, n_heads, dropout, block_size) for _ in range(n_layers)])\n",
        "    self.layer_norm = nn.LayerNorm(n_embedding)\n",
        "    self.linear_head = nn.Linear(n_embedding, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets):\n",
        "    Batch, Time = idx.shape\n",
        "\n",
        "    token_embedding = self.token_embeddding_lookup(idx)\n",
        "    positional_embedding = self.block_embedding_lookup(torch.arange(Time, device=device))\n",
        "    \n",
        "    x = token_embedding + positional_embedding\n",
        "    x = self.transformer_blocks(x)\n",
        "    x = self.layer_norm(x)\n",
        "    \n",
        "    logits = self.linear_head(x)\n",
        "    Batch, Time, Channel = logits.shape\n",
        "    logits = logits.view(Batch*Time, Channel)  #Just changing the view of tensor not copying data\n",
        "    targets = targets.view(Batch*Time) #Just changing the view of tensor not copying data\n",
        "    \n",
        "    loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8EoT7dafwHMD"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1ruQlQB3iqg"
      },
      "source": [
        "Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "I5OKQUvy3iG4"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "block_size = 256\n",
        "epochs = 2400\n",
        "eval_interval = 500\n",
        "learning_rate = 0.1 #3e-4\n",
        "eval_epochs = 200\n",
        "embedding_dim = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "d_EhJJ974UO3"
      },
      "outputs": [],
      "source": [
        "model = NanoGPT(vocab_size=tokenizer.vocab_size,\n",
        "                n_embedding=embedding_dim,\n",
        "                block_size=block_size,\n",
        "                n_heads=n_head,\n",
        "                n_layers=n_layer,\n",
        "                dropout=dropout)\n",
        "\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) #optim.Adam(model.parameters(), lr=learning_rate)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9JNS1aZTwkR0"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_epochs):\n",
        "  out={}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_epochs)\n",
        "    for k in range(eval_epochs):\n",
        "        X, Y = get_batch(split)\n",
        "        logits, loss = model(X, Y)\n",
        "        losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3djK2xEwDxW",
        "outputId": "1fe2ea6f-a32a-43bb-b858-bb1e50fca865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train loss 3.874, Validation loss 3.888\n",
            "Epoch 200: Train loss 2.477, Validation loss 2.497\n",
            "Epoch 400: Train loss 2.452, Validation loss 2.474\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):  \n",
        "  X, y = get_batch('train')\n",
        "  logits, loss = model(X, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % eval_epochs == 0 or epoch == epochs-1:\n",
        "    losses = estimate_loss(model, eval_epochs)\n",
        "    print(f\"Epoch {epoch}: Train loss {losses['train']:.3f}, Validation loss {losses['val']:.3f}\")\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4DqvPjSqd7m3"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model):\n",
        "  accuracy = Accuracy(task='multiclass', num_classes=65, top_k=10).to(device)\n",
        "  model.eval()\n",
        "  out={}\n",
        "  for split in ['train', 'val']:\n",
        "    accuracies = torch.zeros(eval_epochs)\n",
        "    for k in range(eval_epochs):\n",
        "        X, Y = get_batch(split)\n",
        "        logits, _ = model(X, Y)\n",
        "        # logits = torch.argmax(logits, dim=-1)\n",
        "        Y = torch.flatten(Y)\n",
        "        acc = accuracy(logits, Y)\n",
        "        accuracies[k] = acc.item()\n",
        "    out[split] = accuracies.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "accuracies = evaluate_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAImxSi9khVd",
        "outputId": "afa43c67-41c3-43b8-d7db-e4f823d54c49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train top-10 accuracy 0.850, Validation top-10 accuracy 0.845\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train top-10 accuracy {accuracies['train']:.3f}, Validation top-10 accuracy {accuracies['val']:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "C0ct3UCk7HNM",
        "4QrdIBd47T5c",
        "6V2XzJUglP5a",
        "sD_sJV316mjw",
        "5wGOnke66ZLu",
        "A1qlYg0nT03H",
        "jLm4nRYQ6hmY"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
