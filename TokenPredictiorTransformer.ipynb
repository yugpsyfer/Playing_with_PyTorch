{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yugpsyfer/Playing_with_PyTorch/blob/main/TokenPredictiorTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Self Attention\n",
        "\n",
        "I have tried to implement self attention paper from scratch, although I have missed things like positional encoding.\n",
        "\n",
        "What I have implemented:- \\\\\n",
        " * Self Attention\n",
        " * Multihead attention\n",
        " * Encoder and Decoder"
      ],
      "metadata": {
        "id": "2h4EsnG1EZ-e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcU2vymJppG6"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dbQQ-npEpJEC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import dataloader, Dataset\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qndDz9W1uji_"
      },
      "source": [
        "##Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HImC59mj2eW2"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/Datasets/internet_archive_scifi_v3.txt\"\n",
        "corpus = []\n",
        "vocab_size = 128\n",
        "\n",
        "def data_reader():\n",
        "    corpus = []\n",
        "    with open(data_path, 'r') as fp:\n",
        "        train_data = fp.readlines()\n",
        "        corpus += train_data[0].split('.')\n",
        "    return corpus, train_data\n",
        "\n",
        "\n",
        "corpus, train_data = data_reader()\n",
        "with open(\"text_data_text.txt\", 'a') as wp:\n",
        "    for line in corpus:\n",
        "        wp.write(line + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpPU0rhzunH4"
      },
      "source": [
        "##Tokenizer\n",
        "\n",
        "Directly used google's sentence piece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BnMUeVFB72h5"
      },
      "outputs": [],
      "source": [
        "spm.SentencePieceTrainer.train(input=\"/content/text_data_text.txt\", model_prefix='tokenizer', vocab_size=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l2Y1AiZLJHiw"
      },
      "outputs": [],
      "source": [
        "sp_e = spm.SentencePieceProcessor(model_file=\"/content/tokenizer.model\")\n",
        "\n",
        "tokenized_data = sp_e.encode(train_data[0][0:12800])\n",
        "data_length = len(tokenized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D2SKMsqAJFz",
        "outputId": "6d0f9f4e-d571-4408-8b67-f16554d63181"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8430"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYUqhqeiuiMo"
      },
      "source": [
        "##Creating Input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v_8aQK0rynUe"
      },
      "outputs": [],
      "source": [
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "num_batches = data_length//(20*64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "70dyKCUcux0G"
      },
      "outputs": [],
      "source": [
        "def make_batch():\n",
        "    batches = []\n",
        "    for batch in range(num_batches):\n",
        "        x = []\n",
        "        y = []\n",
        "        start = random.randint(0,data_length)\n",
        "        end = start+sequence_length\n",
        "\n",
        "        if data_length < end:\n",
        "            continue\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            x += torch.tensor([tokenized_data[start:end]], dtype=torch.long)\n",
        "            y += torch.tensor([tokenized_data[start+1:end+1]], dtype=torch.long)\n",
        "\n",
        "        x = torch.stack(x)\n",
        "        y = torch.stack(y)\n",
        "\n",
        "        batches.append((x,y))\n",
        "\n",
        "    return batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oZoUQOp1BC7C"
      },
      "outputs": [],
      "source": [
        "batches = make_batch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa3_rD5C0xMJ"
      },
      "source": [
        "##Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "uK9m9LtpxeQO"
      },
      "outputs": [],
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, masking=False):\n",
        "        super().__init__()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.masking = masking\n",
        "\n",
        "    def mask(self, x):\n",
        "        lower_tri = torch.tril(torch.ones(x.shape))\n",
        "        lower_tri[lower_tri == 0] = -float(\"Inf\")\n",
        "        lower_tri = lower_tri.to('cuda')\n",
        "        x = x @ lower_tri\n",
        "        return x\n",
        "\n",
        "    def forward(self, K, Q, V):\n",
        "        matmul = torch.bmm(Q, torch.transpose(K, 1,2))\n",
        "        scaled = matmul/math.sqrt(K.shape[2])\n",
        "\n",
        "        if self.masking:\n",
        "            self.mask(scaled)\n",
        "\n",
        "        return self.softmax(scaled) @ V"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads=4, masking=False):\n",
        "        super().__init__()\n",
        "        self.attention_heads = heads\n",
        "        self.multi_attention = nn.ModuleList([SingleHeadAttention(masking=masking) for h in range(heads)])\n",
        "\n",
        "    def forward(self, K, Q, V):\n",
        "        b,t,d = K.shape\n",
        "        step = d//self.attention_heads\n",
        "\n",
        "        out = [h(K[:,:, idx*step : (idx+1)*step], Q[:,:, idx*step : (idx+1)*step], V[:,:, idx*step : (idx+1)*step]) for idx, h in enumerate(self.multi_attention)]\n",
        "        out = torch.concat(out, dim=-1)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "6aerW-_8hZJ2"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "SFeD6xZZ0-n7"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, token_size, proj_dim):\n",
        "        super().__init__()\n",
        "        self.attention_layer = MultiHeadAttention()\n",
        "        self.layer_norm = nn.LayerNorm(token_size)\n",
        "        self.final_linear_layer = nn.Linear(token_size, token_size)\n",
        "        self.final_layer_norm = nn.LayerNorm(token_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.attention_layer(input, input, input)\n",
        "        x = self.layer_norm(x + input)     # Residual connection\n",
        "        out = self.final_linear_layer(x)\n",
        "        out = self.final_layer_norm(out + x)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "YhY3BtMy2Q4V"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, token_size, proj_dim):\n",
        "        super().__init__()\n",
        "        self.masked_attention_layer = MultiHeadAttention(masking=True)\n",
        "        self.layer_norm_1 = nn.LayerNorm(token_size)\n",
        "\n",
        "        self.attention_layer = MultiHeadAttention()\n",
        "        self.layer_norm_2 = nn.LayerNorm(token_size)\n",
        "\n",
        "        self.final_layer_norm = nn.LayerNorm(token_size)\n",
        "        self.final_linear_layer = nn.Linear(token_size, token_size)\n",
        "\n",
        "    def forward(self, x, encoded_input):\n",
        "        out = self.masked_attention_layer(x, x, x)\n",
        "        out = self.layer_norm_1(out + x)\n",
        "\n",
        "        final_out = self.attention_layer(encoded_input, encoded_input, out)\n",
        "        final_out = self.layer_norm_2(final_out + out)\n",
        "        out = self.final_linear_layer(final_out)\n",
        "\n",
        "        return self.final_layer_norm(out + final_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3SJ55cGU2PAD"
      },
      "outputs": [],
      "source": [
        "class TokenPredictor(nn.Module):\n",
        "    def __init__(self, token_size, tokens, proj_dim=512):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(token_size, proj_dim)\n",
        "        self.decoder = Decoder(token_size, proj_dim)\n",
        "\n",
        "        self.prediction_head = nn.Sequential(\n",
        "            nn.Linear(token_size, tokens),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "\n",
        "        encoded_out = self.encoder(inp)\n",
        "        out = self.decoder(inp, encoded_out)\n",
        "\n",
        "        return self.prediction_head(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvYE0ryOPIgN"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Tf8Otu5kJYhN"
      },
      "outputs": [],
      "source": [
        "token_size = 48\n",
        "epochs = 3000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda'\n",
        "\n",
        "model = TokenPredictor(token_size=token_size, tokens=vocab_size, proj_dim=256)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "embedding_fn = nn.Embedding(num_embeddings=vocab_size,embedding_dim=token_size)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufTKWlfv2U1b",
        "outputId": "60181ff4-781e-47bc-c5eb-a3feddb5d3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.851099014282227 epoch: 0\n",
            "Loss: 4.203580379486084 epoch: 100\n",
            "Loss: 4.176867485046387 epoch: 200\n",
            "Loss: 4.175509452819824 epoch: 300\n",
            "Loss: 4.174942970275879 epoch: 400\n",
            "Loss: 4.174624443054199 epoch: 500\n",
            "Loss: 4.166080474853516 epoch: 600\n",
            "Loss: 4.1658034324646 epoch: 700\n",
            "Loss: 4.157221794128418 epoch: 800\n",
            "Loss: 4.156882286071777 epoch: 900\n",
            "Loss: 4.156682014465332 epoch: 1000\n",
            "Loss: 4.156554222106934 epoch: 1100\n",
            "Loss: 4.156494140625 epoch: 1200\n",
            "Loss: 4.1563591957092285 epoch: 1300\n",
            "Loss: 4.152240753173828 epoch: 1400\n",
            "Loss: 4.152204513549805 epoch: 1500\n",
            "Loss: 4.152155876159668 epoch: 1600\n",
            "Loss: 4.152143478393555 epoch: 1700\n",
            "Loss: 4.152100563049316 epoch: 1800\n",
            "Loss: 4.1520490646362305 epoch: 1900\n",
            "Loss: 4.15201997756958 epoch: 2000\n",
            "Loss: 4.152017116546631 epoch: 2100\n",
            "Loss: 4.151995658874512 epoch: 2200\n",
            "Loss: 4.151968002319336 epoch: 2300\n",
            "Loss: 4.1519455909729 epoch: 2400\n",
            "Loss: 4.151934623718262 epoch: 2500\n",
            "Loss: 4.151923656463623 epoch: 2600\n",
            "Loss: 4.151916027069092 epoch: 2700\n",
            "Loss: 4.151909828186035 epoch: 2800\n",
            "Loss: 4.151906967163086 epoch: 2900\n"
          ]
        }
      ],
      "source": [
        "def train():\n",
        "    for eps in range(epochs):\n",
        "        Loss = 0\n",
        "        for x,target in batches:\n",
        "            x = embedding_fn(x)\n",
        "            b,t,d = x.shape\n",
        "            target = target.reshape(b*t)\n",
        "            x = x.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            out = model(x).reshape(b*t,-1)\n",
        "            loss = loss_fn(out, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            Loss += loss\n",
        "\n",
        "        if eps%100 == 0:\n",
        "            print(\"Loss: {} epoch: {}\".format(Loss/num_batches, eps))\n",
        "\n",
        "\n",
        "train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "19x-7AHEsbwjY02v1NxbcxFOoshG_TUWA",
      "authorship_tag": "ABX9TyNz7fPdKPEjVBCoSsj+n2MT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}