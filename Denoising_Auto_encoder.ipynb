{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPYHnAhhxVw9HcXWvgwfHMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yugpsyfer/Playing_with_PyTorch/blob/main/Denoising_Auto_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1GKASotGG3Ya"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as ds\n",
        "from torchvision.transforms import v2,ToTensor, Resize, functional\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transf = v2.Compose([ToTensor(), Resize(size=(128,128))])\n",
        "\n",
        "flowers = ds.Flowers102(root='./', split=\"train\" ,download=True,\n",
        "                        transform=transf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbp67duLMh2R",
        "outputId": "95d87db5-6327-4024-ec4e-e1765953cc38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 344862509/344862509 [00:16<00:00, 20488546.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting flowers-102/102flowers.tgz to flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to flowers-102/imagelabels.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 502/502 [00:00<00:00, 446846.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to flowers-102/setid.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14989/14989 [00:00<00:00, 18474411.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "loader = DataLoader(dataset=flowers, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "PpyuY4CwNDx2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder - Decoder\n",
        "\n",
        "*Encoder output of layers:-*  \n",
        "\n",
        "Height output and Width output are same in our case.\n",
        "\n",
        "###Convolution Layers\n",
        "* Hout=(Hin−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1\n",
        "\n",
        "###Max Pooling\n",
        "* Hout=⌊(Hin+2∗padding[0]−dilation[0]×(kernel_size[0]−1)−1)/stride[0]\n",
        " +1⌋\n",
        "\n",
        "###Transposed Conv Layer:\n",
        "\n",
        "* (H-1) * stride - 2 * padding + dilation * (kernel_size-1)+output_padding+1"
      ],
      "metadata": {
        "id": "4KCzbZ4IdF2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3,3)),\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3,3)),\n",
        "            nn.MaxPool2d(kernel_size=(3,3)),80\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3)),\n",
        "            nn.MaxPool2d(kernel_size=(3,3)),\n",
        "        )\n",
        "\n",
        "        self.encoded_rep = nn.Sequential(\n",
        "            nn.Linear(5408, 5408),\n",
        "            nn.Linear(5408, 2048),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=(7,7), stride=3, output_padding=2, dilation=3),\n",
        "            nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=(7,7), stride=2, output_padding=1, dilation=3),\n",
        "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=(7,7), stride=1, output_padding=0, dilation=2),\n",
        "            nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=(5,5), stride=1, dilation=2),\n",
        "            nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=(5,5), stride=1),\n",
        "            nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=(3,3))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder(x)\n",
        "\n",
        "        b, f, m, n = out.shape\n",
        "        out = self.encoded_rep(out.view(b,f*m*n))\n",
        "\n",
        "        return self.decoder(out.view(b,32,8,8))"
      ],
      "metadata": {
        "id": "YoZkC3QWREEm"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "weight_decay = 0.03\n",
        "device = \"cuda\"\n",
        "\n",
        "model = AutoEncoder()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "auqvZfTfdT18"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def validation(model, dataloader, val_images=10, path='./out/'):\n",
        "    count = 0\n",
        "    for batch in dataloader:\n",
        "        x,_ = batch\n",
        "        x = x.to(device)\n",
        "        out = model(x)\n",
        "        b,c,m,n = out.shape\n",
        "        INPUT = functional.to_pil_image(x[0,:,:,:].squeeze())\n",
        "        INPUT.save(path + 'orig_img_'+str(count)+'.jpg')\n",
        "        TARGET = functional.to_pil_image(out[0,:,:,:].squeeze())\n",
        "        TARGET.save(path + 'img_' + str(count)+'.jpg')\n",
        "        count+=1\n",
        "        if count > val_images:\n",
        "            return"
      ],
      "metadata": {
        "id": "-LyEfxNYRzyw"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epochs, dataloader):\n",
        "    for eps in range(epochs):\n",
        "        LOSS = 0\n",
        "        for batch in dataloader:\n",
        "            x, _ = batch\n",
        "            x = x.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = loss_fn(out, x)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            LOSS+=loss\n",
        "\n",
        "        print(\"EPOCH:{} | LOSS:{}\".format(eps,LOSS))\n",
        "\n",
        "\n",
        "\n",
        "train(model, 100, loader)"
      ],
      "metadata": {
        "id": "nAq_f9r7RwZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation(model, loader)"
      ],
      "metadata": {
        "id": "3xThMnNPlp2Z"
      },
      "execution_count": 148,
      "outputs": []
    }
  ]
}